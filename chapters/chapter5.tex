\section{Gradient Boosting (5-6 Seiten)}
Der Gradient Boosting Trees (GBT) Algorithmus hat sich in den letzten Jahren als einer der führenden Boosting-Algorithmen etabliert. Charakteristisch für GBT ist die Verwendung von Entscheidungsbäumen als Basis-Lernalgorithmus. Ähnlich wie AdaBoost verfolgt GBT das Prinzip des adaptiven Lernens (\autoref{sec:adaptive_learning}), bei dem jeder nachfolgende Entscheidungsbaum darauf abzielt, die Fehler seines Vorgängers zu korrigieren.

\subsection{Unterschied zu AdaBoost}
Der entscheidende Unterschied zu AdaBoost \ref{sec:adaptive_learning} liegt jedoch in der Art und Weise, wie GBT die Trainingsdaten behandelt. Während AdaBoost die Gewichtung aller Trainingsdaten anpasst, um die Fehler des vorherigen Lerners hervorzuheben, beschränkt sich GBT auf die Redzierung des Restfehlers.
Genauer gesagt bedeutet dies, dass jeder neue Baum auf den Residuen (Unterschiede zwischen den tatsächlichen und den vorhergesagten Werten) des vorherigen trainiert wird.
\newline
Obwohl GBT sowohl für Klassifikations- als auch für Regressionsprobleme einsetzbar ist, konzentriert sich diese Arbeit hauptsächlich auf die Anwendung von GBT im Kontext der Regression. 
Der Grund dafür ist, dass GBT deutlich einfach verständlich ist in der Art und Weise, wie er kontinuierliche Werte vorhersagt.
\newline

\subsection{Die Loss-Funktion \( L \)}
Ein wesentlicher Bestandteil des Gradient Boosting Trees (GBT) Algorithmus ist die Loss-Funktion, die zur Bewertung der Modellgenauigkeit eingesetzt wird. In GBT wird häufig die Mean Squared Error (MSE) Loss-Funktion verwendet. Diese berechnet die quadratische Differenz zwischen den tatsächlichen Werten \( y \) und den vom Modell vorhergesagten Werten \( F(x) \):

\begin{equation}
    L(y, F(x)) = \frac{1}{2}(y - F(x))^2
\end{equation}

Die Division durch 2 dient der Vereinfachung der Berechnung des Gradienten, welcher für die Anpassung des Modells benötigt wird.

Ein zentrales Konzept im GBT ist das der Residuen. Residuen bezeichnen die Differenz zwischen den tatsächlichen Datenpunkten \( y_i \) und den durch das Modell vorhergesagten Werten \( F(x_i) \). Sie geben Aufschluss darüber, wie weit das aktuelle Modell von den realen Daten abweicht.

Im Kontext des GBT spricht man oft von Pseudo-Residuen. Diese werden für jede Iteration \( m \) des Algorithmus berechnet und sind definiert als die negative Ableitung der Loss-Funktion bezüglich der Modellvorhersagen:

\begin{equation}
    r_{i,m} = -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\bigg|_{F(x)=F_{m-1}(x)}
\end{equation}

Für die MSE Loss-Funktion vereinfacht sich dies zu:

\begin{equation}
    r_{i,m} = y_i - F_{m-1}(x_i)
\end{equation}

Das Pseudo-Residuum repräsentiert somit den Fehler des aktuellen Modells \( F_{m-1}(x) \) bei der Vorhersage des tatsächlichen Wertes \( y_i \). Im Gegensatz zum einfachen Residuum, welches nur den Fehler angibt, reflektiert das Pseudo-Residuum die Richtung und das Ausmaß der erforderlichen Anpassung des Modells. Dies ist entscheidend für den GBT-Algorithmus, da in jedem Schritt ein neuer Entscheidungsbaum trainiert wird, um genau diese Pseudo-Residuen zu minimieren.


















\subsection{Algorithmus-Struktur und Funktionsweise}
Der GBT-Algorithmus (siehe \autoref{algo:gbt} ist in mehrere Schritte unterteilt, welche im folgenden genau erklärt werden:


\subsection{Algorithmus-Struktur und Funktionsweise}
Der Gradient Boosting Trees Algorithmus, im Folgenden GBT genannt, ist ein komplexer, aber hochgradig effektiver Boosting-Algorithmus. Der folgende Abschnitt erklärt die einzelnen Schritte des GBT-Algorithmus (siehe Algorithmus~\ref{algo:gbt}):

\paragraph{Schritt 1: Initialisierung (Zeile 2)}\label{para:GBT_Initialisierung}
\label{para:gbt_algo_s1}
Zu Beginn wird ein Basismodell \( F_0(x) \) erstellt, welches oft der Durchschnitt oder Median der Zielwerte \( y \) ist. Dieses Modell bildet den Ausgangspunkt für die iterativen Verbesserungen.

\paragraph{Schritt 2: Iterationen (Zeile 3 bis 7)}
\label{para:gbt_algo_s2}
Im Kern des GBT-Algorithmus stehen iterative Schritte, die darauf abzielen, das aktuelle Modell schrittweise zu verbessern. Diese Iterationen laufen über einen vordefinierten Zeitraum \( T \).

\paragraph{Schritt 2.1: Berechnung der Residuen (Zeile 4)}
\label{para:gbt_algo_s2.1}
In jeder Iteration \( t \) werden die Residuen \( r_{t,i} \) für jeden Datenpunkt \( i \) berechnet. Diese Residuen repräsentieren die Differenz zwischen den tatsächlichen Werten \( y_i \) und den Vorhersagen des aktuellen Modells \( F_{t-1}(x_i) \).

\paragraph{Schritt 2.2: Training der Entscheidungsbäume (Zeile 5)}
\label{para:gbt_algo_s2.2}
Ein neuer Entscheidungsbaum \( h_t \) wird auf den Residuen trainiert. Jeder Baum fokussiert sich darauf, die Fehler des vorherigen Modells zu minimieren.

\paragraph{Schritt 2.3: Optimierung des Multiplikators (Zeile 6)}
\label{para:gbt_algo_s2.3}
Für jeden Baum wird ein Multiplikator \( \gamma_t \) gefunden, der den Einfluss des Baumes auf das Gesamtmodell optimal anpasst. Dies kann beispielsweise durch eine Linearsuche erfolgen.

\paragraph{Schritt 2.4: Aktualisierung des Modells (Zeile 7)}
\label{para:gbt_algo_s2.4}
Das Modell wird durch Hinzufügen des neuen, gewichteten Entscheidungsbaumes \( \gamma_t h_t(x) \) zum aktuellen Modell \( F_{t-1}(x) \) aktualisiert. Dadurch wird das Modell iterativ verbessert.

\paragraph{Schritt 3: Ausgabe des finalen Modells (Zeile 8)}
\label{para:gbt_algo_s3}
Nach Abschluss aller Iterationen wird das finale Modell \( F_T(x) \) als das Ergebnis des GBT-Algorithmus ausgegeben. Dieses Modell ist die Summe aller gewichteten Entscheidungsbäume und repräsentiert die kumulativen Verbesserungen über alle Iterationen.

\begin{algorithm}[H]
    \caption[Gradient Boosting Trees Algorithmus]{Gradient Boosting Trees Algorithmus (nach \textcite[S.~346]{Frochte2020})}\label{algo:gbt}
    \begin{algorithmic}[1]
    \State \textbf{Gegeben:} Trainingsdatensatz \( (x_1,y_1), \dots, (x_m,y_m) \)
    \State \textbf{Initialisierung:} Initialisiere das Modell mit einem konstanten Wert \( F_0(x) = \frac{1}{n} \sum_{i=1}^{n} y_i \)
    \For{\( t = 1, \dots, T \)}
        \State Berechne Residuen \( r_{t,i} = y_i - F_{t-1}(x_i) \) für alle \( i \)
        \State Trainiere einen Entscheidungsbaum \( h_t \) auf \( (x_i, r_{t,i}) \)
        \State Finde den besten Multiplikator \( \alpha_t \) für \( h_t(x) \), z.B. durch Linearsuche
        \State Aktualisiere das Modell: \( F_t(x) = F_{t-1}(x) + \alpha_t \cdot h_t(x) \)
    \EndFor
    \State \textbf{Ausgabe:} Das endgültige Modell \( F_T(x) \)
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \caption[MSE Gradient Tree Boosting]{MSE Gradient Tree Boosting (nach \textcite[S.~346]{Frochte2020})}\label{algo:gbt}
    \begin{algorithmic}[1]
    \State \textbf{Gegeben:} Trainingsdatensatz \( (x_1,y_1), \dots, (x_n,y_n) \)
    \State \textbf{Initialisierung:} Starte mit einem Basismodell \( F_0(x) \), z.B. dem Mittelwert der Labels \( y \)
    \For{\( m = 1, \dots, M \)}
        \State Berechne das Pseudo-Residuum \( r_{i,m} = y_i - F_{m-1}(x_i) \) für alle \( i \)
        \State Trainiere einen Entscheidungsbaum \( h_m \) auf \( (x_i, r_{i,m}) \)
        \State Führe das Update durch: \( F_m(x) = F_{m-1}(x) + \alpha \cdot h_m(x) \)
    \EndFor
    \State \textbf{Ausgabe:} Das endgültige Modell \( F_M(x) \)
    \end{algorithmic}
\end{algorithm}





\subsection{Beispielanwendung mit Erläuterung}
