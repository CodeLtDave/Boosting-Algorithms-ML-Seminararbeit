\section{Einleitung}
Die voranschreitende Digitalisierung unserer Gesellschaft und die daraus folgende exponentielle Zunahme an Daten, hat das maschinelle Lernen als Schlüsselelement der modernen Datenanalyse etabliert. In den vielen Teilgebieten des maschinellen Lernens haben Boosting-Algorithmen einen besonderen Stellenwert. Besonders auf tabellarischen Datensätzen erbringen sie in vergleichsweise kurzer Trainingszeit in fast allen Benchmarks die besten Ergebnisse. Diese Seminararbeit konzentriert sich auf zwei Vertreter der Boosting-Familie: AdaBoost und Gradient Boosting Trees. Inhaltlich wird die Seminararbeit darauf abzielen, diese Algorithmen zu untersuchen und anhand eigener Beispiele zu erklären.

\subsection{Hintergrund und Motivation}
In der heutigen Zeit gibt es ständige Fortschritte im Bereich künstlicher Intelligenz. Nicht zuletzt mit der Veröffentlichung von ChatGPT ist die Benutzung von KIs zum Alltag geworden und wird auch von der breiten Masse genutzt. Als Student im Bereich der Informatik kann man es sich nicht mehr leisten, sich nicht mit den Themen der künstlichen Intelligenz zu beschäftigen. Die Fortschritte im Bereich der KI, wie sie ChatGPT demonstriert, haben sogar die lang angenommene Sicherheit des Berufs als Softwareentwickler beeinflusst. Zwar ist meine Einschätzung, dass KI-Systeme in naher Zukunft vorrangig als Werkzeuge benutzt werden und keine Berufsgruppen komplett ersetzen, doch selbst wenn ich falsch liege, ist davon auszugehen, dass diejenigen, die solche Tools entwickeln, als letzte ersetzt werden. Diese Überlegungen zur Zukunftssicherheit und die Erwartung, dass bahnbrechende Entwicklungen vor allem im KI-Bereich stattfinden werden, fördern mein Interesse schon seit längerem. Die Wahl des Themas dieser Seminararbeit spiegelt also mein Interesse an KI wieder und bietet eine gute Gelegenheit um sich mit Boosting-Algorithmen auseinanderzusetzten, was mir sicherlich in Zukunft zu gute kommen wird.

\subsection{AdaBoost und GradientBoosting: Ein Überblick}
AdaBoost war eins der ersten Beispiele für Boosting und wird als Stellvertreter für diese Kategorie gesehen. Die Grundidee ist in einem iterativen Prozess des Trainings die Gewichtung der Daten zu ändern, sodass sie sich auf die Fehler fokussieren. Ähnlich dazu ist Gradient Boosting Trees, wobei dieser Algorithmus Entscheidungsbäume nutzt, welche auf dem Restfehler des Vorgängers trainiert werden. Beide Algorithmen nutzen das Prinzip der iterativen Verbesserung, allerdings auf unterschiedliche Weise, was ihre Anwendung in verschiedenen Kontexten und Aufgabentypen interessant macht.

\subsection{Zielsetzung und Struktur der Arbeit}
Diese Arbeit zielt darauf ab, die beiden genannten Algorithmen umfassend zu analysieren. Dabei wird ein Schwerpunkt auf die praktische Anwendung und die Veranschaulichung ihrer Funktionsweise durch eigene Beispiele gelegt. Der Aufbau der Arbeit ist wie folgt strukturiert: Zunächst wird ein grundlegendes Verständnis des Boosting-Konzepts geschaffen. Anschließend werden die Algorithmen AdaBoost und Gradient Boosting Trees detailliert beschrieben, wobei deren theoretische Grundlagen, Funktionsweisen im Vordergrund stehen, aber durch Beispiele veranschaulicht werden. Darauf aufbauend folgt eine direkte Gegenüberstellung beider Algorithmen, in der ihre Unterschiede, Vor- und Nachteile sowie optimale Einsatzgebiete diskutiert werden.