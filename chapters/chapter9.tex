\section{Fazit und Ausblick}
In dieser Arbeit wurde sich intensiv mit den vorgestellten Boosting Algorithmen AdaBoost und Gradient Boosting Trees beschäftigt. Beginnend mit einer Erklärung der theoretischen Funktionsweise und Erläuterung anhand praktischer Anwendung wurden die beiden Algorithmen intensiv untersucht. Anhand des Verständnis über ihre Funktionsweise konnten Unterschiede, Stärken, Schwächen analysiert und erklärt werden.
\newline
AdaBoost's Stärke zeigt sich besonders in seiner Resistenz gegen Overfitting. Auch wenn AdaBoost definitiv zum Overfitting gebracht werden kann, ist es doch eine nützliche Widerstandskraft, im Kontext des maschinellen Lernens. Durch konsequente Fokussierung auf Fehler während des Trainings, bei der die Daten gewichtet werden, gelingt es AdaBoost ein leistungsstarkes Model zu kreieren, was sich besonders für die zweiklassige Klassifikation eignet und sehr effizient arbeitet.  
\newline
GBT hingegen ist durch seine Flexibilität auf viele Probleme anwendbar und kann sich besonders bei Komplexen Problemen hervortun. Durch seine Verwendung von Entscheidungsbäumen und die Fokussierung auf die Minimierung des Restfehlers ist GBT ein leistungsstarker und vielseitig einsetzbarer Vertreter der Boosting-Algorithmen.
\newline
\newline
Abschließend lässt sich sagen, sowohl AdaBoost als auch GBT sind nicht zu unterschätzende ML-Modelle sind. Inzwischen gibt es viele Abwandlungen der Algorithmen, wie beispielsweise XGBoost (eXtreme Gradient Boosting), welche Verfeinerungen bieten, die sowohl neue Anwendungsgebiete ermöglichen, als auch die Leistung erhöhen können. In einer Zeit in der Neuronale Netze und Deep Learning an Popularität gewinnen, behalten AdaBoost und Gradient Boosting Trees (GBT) ihren wichtigen Stellenwert in der Welt des maschinellen Lernens. Sie bieten robuste, effiziente Lösungen für eine Vielzahl von Datenstrukturen und Problemen, vor allem in Bereichen, wo Daten möglicherweise nicht ausreichen für den Einsatz von Deep-Learning-Methoden.